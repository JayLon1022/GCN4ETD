{
 "cells": [
  {
   "cell_type": "code",
   "id": "eba7871008ccd8e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T05:48:48.301064Z",
     "start_time": "2024-12-16T05:48:43.440975Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)  # 第一层图卷积\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)  # 第二层图卷积\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))  # 图卷积 + ReLU\n",
    "        x = F.dropout(x, self.dropout, training=self.training)  # 防止过拟合\n",
    "        x = self.gc2(x, adj)  # 输出层\n",
    "        return F.log_softmax(x, dim=1)  # Log-Softmax 用于多分类任务\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / np.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "dataset = pd.read_csv(\"processed_data.csv\")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "6905282e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T05:48:48.455129Z",
     "start_time": "2024-12-16T05:48:48.308073Z"
    }
   },
   "source": [
    "features_columns = ['Duration', 'Source Port', 'Destination Port', 'Packets', 'Bytes','Label']\n",
    "features = dataset[features_columns]\n",
    "print(features)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Duration  Source Port  Destination Port  Packets  Bytes       Label\n",
      "0             0.0           53             43192        1    214  background\n",
      "1             0.0        60185                53        1     72  background\n",
      "2             0.0        48598                53        1     77  background\n",
      "3             0.0        51465                53        1     63  background\n",
      "4             0.0           80             37934        1     52  background\n",
      "...           ...          ...               ...      ...    ...         ...\n",
      "4999995       0.0        43924             53413        1    151  background\n",
      "4999996       0.0        54339             53413        1    151  background\n",
      "4999997       0.0        43355             53413        1    151  background\n",
      "4999998       0.0        25985                53        1     75  background\n",
      "4999999       0.0           53             58327        1     93  background\n",
      "\n",
      "[5000000 rows x 6 columns]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "d0629aba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T05:48:48.964902Z",
     "start_time": "2024-12-16T05:48:48.505281Z"
    }
   },
   "source": [
    "X = dataset.drop(columns=['Label'])\n",
    "y = dataset['Label']\n",
    "\n",
    "train_data = pd.DataFrame(X, columns=X.columns)\n",
    "train_data['Label'] = y\n",
    "\n",
    "# 查看数据集类别分布\n",
    "print(\"数据集类别分布：\")\n",
    "print(train_data['Label'].value_counts())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集类别分布：\n",
      "Label\n",
      "background      4931787\n",
      "dos               27419\n",
      "blacklist         13770\n",
      "scan44            13025\n",
      "nerisbotnet        6234\n",
      "anomaly-spam       4961\n",
      "scan11             2804\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "6497d336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T05:48:50.896663Z",
     "start_time": "2024-12-16T05:48:48.970908Z"
    }
   },
   "source": [
    "data_0 = dataset[dataset['Label'] == \"background\"].sample(n=20000, random_state=42)\n",
    "data_1 = dataset[dataset['Label'] == \"dos\"].sample(n=5000, random_state=42)\n",
    "data_2 = dataset[dataset['Label'] == \"blacklist\"].sample(n=5000, random_state=42)\n",
    "data_3 = dataset[dataset['Label'] == \"scan44\"].sample(n=5000, random_state=42)\n",
    "data_4 = dataset[dataset['Label'] == \"nerisbotnet\"].sample(n=5000, random_state=42)\n",
    "data_5 = dataset[dataset['Label'] == \"anomaly-spam\"].sample(n=3000, random_state=42)\n",
    "data_6 = dataset[dataset['Label'] == \"scan11\"].sample(n=2000, random_state=42)\n",
    "data = pd.concat([data_0, data_1, data_2, data_3, data_4, data_5, data_6], ignore_index=True)\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# print(data.head())\n",
    "print('数据集类别分布')\n",
    "print(data['Label'].value_counts())\n",
    "# print(data['Source IP'].value_counts)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集类别分布\n",
      "Label\n",
      "background      20000\n",
      "nerisbotnet      5000\n",
      "blacklist        5000\n",
      "dos              5000\n",
      "scan44           5000\n",
      "anomaly-spam     3000\n",
      "scan11           2000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "fc679507",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T05:49:04.276430Z",
     "start_time": "2024-12-16T05:48:50.927225Z"
    }
   },
   "source": [
    "def build_graph(data):\n",
    "    \"\"\"\n",
    "    构建图结构，包括节点特征矩阵和邻接矩阵\n",
    "    \"\"\"\n",
    "    # 提取特征\n",
    "    features_columns = ['Duration', 'Source Port', 'Destination Port', 'Packets', 'Bytes']\n",
    "    features = data[features_columns]\n",
    "\n",
    "    # 转换 IP 地址为数值\n",
    "    data['Source IP'] = data['Source IP'].apply(lambda x: int(''.join(x.split('.'))))\n",
    "    data['Destination IP'] = data['Destination IP'].apply(lambda x: int(''.join(x.split('.'))))\n",
    "\n",
    "    # 归一化特征\n",
    "    scaler = MinMaxScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "\n",
    "    # 使用哈希表加速构图\n",
    "    ip_to_indices = {}\n",
    "    for idx, ip in enumerate(data['Source IP']):\n",
    "        if ip not in ip_to_indices:\n",
    "            ip_to_indices[ip] = []\n",
    "        ip_to_indices[ip].append(idx)\n",
    "\n",
    "    adj_list = []\n",
    "    for idx, dst_ip in enumerate(data['Destination IP']):\n",
    "        if dst_ip in ip_to_indices:\n",
    "            for neighbor_idx in ip_to_indices[dst_ip]:\n",
    "                adj_list.append((idx, neighbor_idx))\n",
    "\n",
    "    # 构造邻接矩阵\n",
    "    rows, cols = zip(*adj_list)\n",
    "    adj = sp.coo_matrix(\n",
    "        (np.ones(len(rows)), (rows, cols)),\n",
    "        shape=(len(data), len(data)),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    return torch.FloatTensor(features), sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "\n",
    "# 稀疏矩阵转 PyTorch 稀疏张量\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "features, adj = build_graph(data)\n"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 50\u001B[0m\n\u001B[0;32m     47\u001B[0m     shape \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mSize(sparse_mx\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39msparse\u001B[38;5;241m.\u001B[39mFloatTensor(indices, values, shape)\n\u001B[1;32m---> 50\u001B[0m features, adj \u001B[38;5;241m=\u001B[39m build_graph(data)\n",
      "Cell \u001B[1;32mIn[12], line 32\u001B[0m, in \u001B[0;36mbuild_graph\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# 构造邻接矩阵\u001B[39;00m\n\u001B[0;32m     31\u001B[0m rows, cols \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39madj_list)\n\u001B[1;32m---> 32\u001B[0m adj \u001B[38;5;241m=\u001B[39m sp\u001B[38;5;241m.\u001B[39mcoo_matrix(\n\u001B[0;32m     33\u001B[0m     (np\u001B[38;5;241m.\u001B[39mones(\u001B[38;5;28mlen\u001B[39m(rows)), (rows, cols)),\n\u001B[0;32m     34\u001B[0m     shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28mlen\u001B[39m(data), \u001B[38;5;28mlen\u001B[39m(data)),\n\u001B[0;32m     35\u001B[0m     dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32\n\u001B[0;32m     36\u001B[0m )\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mFloatTensor(features), sparse_mx_to_torch_sparse_tensor(adj)\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\scipy\\sparse\\_coo.py:59\u001B[0m, in \u001B[0;36m_coo_base.__init__\u001B[1;34m(self, arg1, shape, dtype, copy)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shape \u001B[38;5;241m=\u001B[39m check_shape(shape, allow_1d\u001B[38;5;241m=\u001B[39mis_array)\n\u001B[0;32m     56\u001B[0m idx_dtype \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_index_dtype(coords,\n\u001B[0;32m     57\u001B[0m                                   maxval\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape),\n\u001B[0;32m     58\u001B[0m                                   check_contents\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 59\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoords \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(np\u001B[38;5;241m.\u001B[39marray(idx, copy\u001B[38;5;241m=\u001B[39mcopy, dtype\u001B[38;5;241m=\u001B[39midx_dtype)\n\u001B[0;32m     60\u001B[0m                      \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m coords)\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m getdata(obj, copy\u001B[38;5;241m=\u001B[39mcopy, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhas_canonical_format \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\scipy\\sparse\\_coo.py:59\u001B[0m, in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shape \u001B[38;5;241m=\u001B[39m check_shape(shape, allow_1d\u001B[38;5;241m=\u001B[39mis_array)\n\u001B[0;32m     56\u001B[0m idx_dtype \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_index_dtype(coords,\n\u001B[0;32m     57\u001B[0m                                   maxval\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape),\n\u001B[0;32m     58\u001B[0m                                   check_contents\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 59\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoords \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(np\u001B[38;5;241m.\u001B[39marray(idx, copy\u001B[38;5;241m=\u001B[39mcopy, dtype\u001B[38;5;241m=\u001B[39midx_dtype)\n\u001B[0;32m     60\u001B[0m                      \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m coords)\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m getdata(obj, copy\u001B[38;5;241m=\u001B[39mcopy, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhas_canonical_format \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c0644251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 LabelEncoder 对标签进行编码\n",
    "label_encoder = LabelEncoder()\n",
    "data['Label'] = label_encoder.fit_transform(data['Label'])\n",
    "\n",
    "# 标签现在是整数形式的编码\n",
    "labels = torch.LongTensor(data['Label'].values)\n",
    "\n",
    "# 将数据移动到 GPU\n",
    "features = features.to(device)\n",
    "adj = adj.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "49acce19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "6    200\n",
      "1    200\n",
      "2    200\n",
      "3    200\n",
      "4    200\n",
      "0    200\n",
      "5    200\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "388cbf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集、验证集和测试集\n",
    "idx_train, idx_temp, labels_train, labels_temp = train_test_split(\n",
    "    range(len(labels)), labels, test_size=0.4, stratify=labels, random_state=42\n",
    ")\n",
    "idx_val, idx_test, labels_val, labels_test = train_test_split(\n",
    "    idx_temp, labels_temp, test_size=0.5, stratify=labels_temp, random_state=42\n",
    ")\n",
    "\n",
    "labels = labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b50afac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6], device='cuda:0')\n",
      "tensor([0, 1, 2, 3, 4, 5, 6], device='cuda:0')\n",
      "tensor([0, 1, 2, 3, 4, 5, 6], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(labels[idx_test].unique())\n",
    "print(labels[idx_train].unique())\n",
    "print(labels[idx_val].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3903ce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_train = torch.LongTensor(idx_train)\n",
    "# idx_val = torch.LongTensor(idx_val)\n",
    "# idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "idx_train = torch.LongTensor(idx_train).to(device)\n",
    "idx_val = torch.LongTensor(idx_val).to(device)\n",
    "idx_test = torch.LongTensor(idx_test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8889cd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gcn(adj, features, labels, idx_train, idx_val, nfeat, nhid, nclass, epochs=200, lr=0.01, weight_decay=5e-4, dropout=0.5):\n",
    "    # 初始化模型和优化器\n",
    "    # model = GCN(nfeat=nfeat, nhid=nhid, nclass=nclass, dropout=dropout)\n",
    "    model = GCN(nfeat=nfeat, nhid=nhid, nclass=nclass, dropout=dropout).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 训练模型\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features, adj)  # 前向传播\n",
    "        loss_train = criterion(output[idx_train], labels[idx_train])  # 计算损失\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 验证集评估\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "        loss_val = criterion(output[idx_val], labels[idx_val])\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {loss_train.item():.4f}, Val Loss = {loss_val.item():.4f}, Val Accuracy = {acc_val:.4f}\")\n",
    "    return model\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    \"\"\"计算分类准确率\"\"\"\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def evaluate_gcn(model, adj, features, labels, idx_test):\n",
    "    \"\"\"评估模型\"\"\"\n",
    "    from tabulate import tabulate\n",
    "    \"\"\"评估模型\"\"\"\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    preds = output[idx_test].max(1)[1].type_as(labels)\n",
    "    \n",
    "    # 获取分类报告\n",
    "    report = classification_report(labels[idx_test].cpu().numpy(), preds.cpu().numpy(), output_dict=True)\n",
    "    \n",
    "    # 格式化并打印分类报告\n",
    "    print(\"Test Classification Report:\")\n",
    "    print(tabulate(\n",
    "        [[key] + [f\"{value:.2f}\" for value in metrics.values()] for key, metrics in report.items() if isinstance(metrics, dict)],\n",
    "        headers=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"],\n",
    "        tablefmt=\"grid\"\n",
    "    ))\n",
    "\n",
    "    # 打印 accuracy, macro avg, weighted avg\n",
    "    accuracy = report['accuracy']\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "96b573de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 127.3781, Val Loss = 115.0087, Val Accuracy = 0.3536\n",
      "Epoch 2: Train Loss = 107.1866, Val Loss = 96.0868, Val Accuracy = 0.3536\n",
      "Epoch 3: Train Loss = 87.8084, Val Loss = 75.9729, Val Accuracy = 0.4071\n",
      "Epoch 4: Train Loss = 68.0462, Val Loss = 64.4046, Val Accuracy = 0.3964\n",
      "Epoch 5: Train Loss = 56.3322, Val Loss = 56.4479, Val Accuracy = 0.3429\n",
      "Epoch 6: Train Loss = 47.6187, Val Loss = 47.8621, Val Accuracy = 0.2286\n",
      "Epoch 7: Train Loss = 40.3102, Val Loss = 44.3751, Val Accuracy = 0.1464\n",
      "Epoch 8: Train Loss = 38.7277, Val Loss = 36.4127, Val Accuracy = 0.1464\n",
      "Epoch 9: Train Loss = 32.2485, Val Loss = 34.0831, Val Accuracy = 0.3250\n",
      "Epoch 10: Train Loss = 31.2485, Val Loss = 29.5552, Val Accuracy = 0.3643\n",
      "Epoch 11: Train Loss = 27.7100, Val Loss = 22.9180, Val Accuracy = 0.3214\n",
      "Epoch 12: Train Loss = 21.6726, Val Loss = 22.8997, Val Accuracy = 0.2929\n",
      "Epoch 13: Train Loss = 21.3615, Val Loss = 21.0674, Val Accuracy = 0.2000\n",
      "Epoch 14: Train Loss = 20.4484, Val Loss = 24.3382, Val Accuracy = 0.2179\n",
      "Epoch 15: Train Loss = 23.8923, Val Loss = 25.5650, Val Accuracy = 0.2179\n",
      "Epoch 16: Train Loss = 25.2681, Val Loss = 23.1547, Val Accuracy = 0.2179\n",
      "Epoch 17: Train Loss = 23.1108, Val Loss = 18.0831, Val Accuracy = 0.2107\n",
      "Epoch 18: Train Loss = 18.3436, Val Loss = 14.5214, Val Accuracy = 0.2036\n",
      "Epoch 19: Train Loss = 14.8363, Val Loss = 13.1028, Val Accuracy = 0.2536\n",
      "Epoch 20: Train Loss = 13.0822, Val Loss = 11.3145, Val Accuracy = 0.2929\n",
      "Epoch 21: Train Loss = 11.0734, Val Loss = 12.0714, Val Accuracy = 0.3071\n",
      "Epoch 22: Train Loss = 12.0421, Val Loss = 11.5881, Val Accuracy = 0.3107\n",
      "Epoch 23: Train Loss = 11.9892, Val Loss = 10.3028, Val Accuracy = 0.2214\n",
      "Epoch 24: Train Loss = 10.9429, Val Loss = 9.4314, Val Accuracy = 0.3071\n",
      "Epoch 25: Train Loss = 10.0108, Val Loss = 8.2407, Val Accuracy = 0.3393\n",
      "Epoch 26: Train Loss = 8.8474, Val Loss = 6.3244, Val Accuracy = 0.3321\n",
      "Epoch 27: Train Loss = 7.2047, Val Loss = 5.9906, Val Accuracy = 0.2429\n",
      "Epoch 28: Train Loss = 6.9534, Val Loss = 6.1592, Val Accuracy = 0.2393\n",
      "Epoch 29: Train Loss = 6.9709, Val Loss = 3.7443, Val Accuracy = 0.3679\n",
      "Epoch 30: Train Loss = 4.5219, Val Loss = 4.6425, Val Accuracy = 0.3500\n",
      "Epoch 31: Train Loss = 5.0969, Val Loss = 4.2424, Val Accuracy = 0.3357\n",
      "Epoch 32: Train Loss = 4.4942, Val Loss = 3.6850, Val Accuracy = 0.2214\n",
      "Epoch 33: Train Loss = 3.7361, Val Loss = 4.5628, Val Accuracy = 0.3071\n",
      "Epoch 34: Train Loss = 4.5445, Val Loss = 3.8735, Val Accuracy = 0.2964\n",
      "Epoch 35: Train Loss = 3.9644, Val Loss = 2.8629, Val Accuracy = 0.3464\n",
      "Epoch 36: Train Loss = 2.9376, Val Loss = 3.1767, Val Accuracy = 0.3893\n",
      "Epoch 37: Train Loss = 3.3901, Val Loss = 4.0556, Val Accuracy = 0.4071\n",
      "Epoch 38: Train Loss = 4.3105, Val Loss = 3.3218, Val Accuracy = 0.4786\n",
      "Epoch 39: Train Loss = 3.4751, Val Loss = 2.9525, Val Accuracy = 0.4786\n",
      "Epoch 40: Train Loss = 3.1616, Val Loss = 3.2202, Val Accuracy = 0.4179\n",
      "Epoch 41: Train Loss = 3.3759, Val Loss = 3.4588, Val Accuracy = 0.3607\n",
      "Epoch 42: Train Loss = 3.4908, Val Loss = 3.2286, Val Accuracy = 0.3571\n",
      "Epoch 43: Train Loss = 3.2584, Val Loss = 3.7459, Val Accuracy = 0.4321\n",
      "Epoch 44: Train Loss = 3.6696, Val Loss = 3.7367, Val Accuracy = 0.4143\n",
      "Epoch 45: Train Loss = 3.7497, Val Loss = 3.0200, Val Accuracy = 0.4143\n",
      "Epoch 46: Train Loss = 3.2212, Val Loss = 2.9883, Val Accuracy = 0.3357\n",
      "Epoch 47: Train Loss = 3.2924, Val Loss = 2.8576, Val Accuracy = 0.3464\n",
      "Epoch 48: Train Loss = 2.9627, Val Loss = 2.4549, Val Accuracy = 0.3607\n",
      "Epoch 49: Train Loss = 2.5895, Val Loss = 2.8580, Val Accuracy = 0.4429\n",
      "Epoch 50: Train Loss = 3.0674, Val Loss = 2.6619, Val Accuracy = 0.4143\n",
      "Epoch 51: Train Loss = 2.8227, Val Loss = 3.6439, Val Accuracy = 0.3286\n",
      "Epoch 52: Train Loss = 3.8241, Val Loss = 2.4881, Val Accuracy = 0.4071\n",
      "Epoch 53: Train Loss = 2.6442, Val Loss = 2.7251, Val Accuracy = 0.4429\n",
      "Epoch 54: Train Loss = 2.8598, Val Loss = 3.3362, Val Accuracy = 0.3964\n",
      "Epoch 55: Train Loss = 3.3290, Val Loss = 2.8615, Val Accuracy = 0.3607\n",
      "Epoch 56: Train Loss = 2.7536, Val Loss = 2.2066, Val Accuracy = 0.4571\n",
      "Epoch 57: Train Loss = 2.2755, Val Loss = 2.9948, Val Accuracy = 0.3107\n",
      "Epoch 58: Train Loss = 3.1667, Val Loss = 2.9539, Val Accuracy = 0.4071\n",
      "Epoch 59: Train Loss = 3.0113, Val Loss = 2.9704, Val Accuracy = 0.4357\n",
      "Epoch 60: Train Loss = 2.9157, Val Loss = 2.3486, Val Accuracy = 0.4071\n",
      "Epoch 61: Train Loss = 2.3106, Val Loss = 2.4474, Val Accuracy = 0.3179\n",
      "Epoch 62: Train Loss = 2.3738, Val Loss = 2.5328, Val Accuracy = 0.3036\n",
      "Epoch 63: Train Loss = 2.4797, Val Loss = 2.5369, Val Accuracy = 0.3250\n",
      "Epoch 64: Train Loss = 2.5456, Val Loss = 2.0904, Val Accuracy = 0.4036\n",
      "Epoch 65: Train Loss = 2.1735, Val Loss = 1.9142, Val Accuracy = 0.4214\n",
      "Epoch 66: Train Loss = 1.8412, Val Loss = 2.0214, Val Accuracy = 0.3893\n",
      "Epoch 67: Train Loss = 2.0846, Val Loss = 1.9771, Val Accuracy = 0.3607\n",
      "Epoch 68: Train Loss = 2.1442, Val Loss = 2.0115, Val Accuracy = 0.4607\n",
      "Epoch 69: Train Loss = 2.1167, Val Loss = 2.0751, Val Accuracy = 0.4607\n",
      "Epoch 70: Train Loss = 2.1073, Val Loss = 1.9420, Val Accuracy = 0.5036\n",
      "Epoch 71: Train Loss = 1.9404, Val Loss = 1.6635, Val Accuracy = 0.5321\n",
      "Epoch 72: Train Loss = 1.6455, Val Loss = 1.7158, Val Accuracy = 0.5286\n",
      "Epoch 73: Train Loss = 1.8068, Val Loss = 1.8005, Val Accuracy = 0.5321\n",
      "Epoch 74: Train Loss = 1.9739, Val Loss = 1.6461, Val Accuracy = 0.5964\n",
      "Epoch 75: Train Loss = 1.7593, Val Loss = 1.6414, Val Accuracy = 0.4643\n",
      "Epoch 76: Train Loss = 1.5751, Val Loss = 1.6853, Val Accuracy = 0.5929\n",
      "Epoch 77: Train Loss = 1.6642, Val Loss = 1.7102, Val Accuracy = 0.5143\n",
      "Epoch 78: Train Loss = 1.7452, Val Loss = 1.5593, Val Accuracy = 0.5393\n",
      "Epoch 79: Train Loss = 1.6380, Val Loss = 1.4642, Val Accuracy = 0.5929\n",
      "Epoch 80: Train Loss = 1.5043, Val Loss = 1.5124, Val Accuracy = 0.5857\n",
      "Epoch 81: Train Loss = 1.5828, Val Loss = 1.4270, Val Accuracy = 0.5536\n",
      "Epoch 82: Train Loss = 1.5579, Val Loss = 1.4061, Val Accuracy = 0.6036\n",
      "Epoch 83: Train Loss = 1.4650, Val Loss = 1.4030, Val Accuracy = 0.5679\n",
      "Epoch 84: Train Loss = 1.4511, Val Loss = 1.4874, Val Accuracy = 0.5643\n",
      "Epoch 85: Train Loss = 1.5361, Val Loss = 1.3935, Val Accuracy = 0.5321\n",
      "Epoch 86: Train Loss = 1.4651, Val Loss = 1.3627, Val Accuracy = 0.5821\n",
      "Epoch 87: Train Loss = 1.4176, Val Loss = 1.3822, Val Accuracy = 0.5607\n",
      "Epoch 88: Train Loss = 1.5098, Val Loss = 1.3797, Val Accuracy = 0.5607\n",
      "Epoch 89: Train Loss = 1.4713, Val Loss = 1.4261, Val Accuracy = 0.4571\n",
      "Epoch 90: Train Loss = 1.4708, Val Loss = 1.5276, Val Accuracy = 0.5607\n",
      "Epoch 91: Train Loss = 1.5230, Val Loss = 1.6009, Val Accuracy = 0.5643\n",
      "Epoch 92: Train Loss = 1.5936, Val Loss = 1.4802, Val Accuracy = 0.5679\n",
      "Epoch 93: Train Loss = 1.5398, Val Loss = 1.3249, Val Accuracy = 0.5714\n",
      "Epoch 94: Train Loss = 1.4416, Val Loss = 1.2357, Val Accuracy = 0.6036\n",
      "Epoch 95: Train Loss = 1.2814, Val Loss = 1.4051, Val Accuracy = 0.5250\n",
      "Epoch 96: Train Loss = 1.5818, Val Loss = 1.3017, Val Accuracy = 0.6036\n",
      "Epoch 97: Train Loss = 1.4334, Val Loss = 1.3210, Val Accuracy = 0.4821\n",
      "Epoch 98: Train Loss = 1.3737, Val Loss = 1.4565, Val Accuracy = 0.6000\n",
      "Epoch 99: Train Loss = 1.4288, Val Loss = 1.6138, Val Accuracy = 0.6000\n",
      "Epoch 100: Train Loss = 1.5634, Val Loss = 1.4918, Val Accuracy = 0.6000\n",
      "Epoch 101: Train Loss = 1.4864, Val Loss = 1.2492, Val Accuracy = 0.6143\n",
      "Epoch 102: Train Loss = 1.3037, Val Loss = 1.4472, Val Accuracy = 0.5107\n",
      "Epoch 103: Train Loss = 1.4852, Val Loss = 1.3759, Val Accuracy = 0.5571\n",
      "Epoch 104: Train Loss = 1.5944, Val Loss = 1.3128, Val Accuracy = 0.6143\n",
      "Epoch 105: Train Loss = 1.4494, Val Loss = 1.5156, Val Accuracy = 0.4929\n",
      "Epoch 106: Train Loss = 1.6126, Val Loss = 1.3459, Val Accuracy = 0.6000\n",
      "Epoch 107: Train Loss = 1.3829, Val Loss = 1.6148, Val Accuracy = 0.5750\n",
      "Epoch 108: Train Loss = 1.5881, Val Loss = 1.4679, Val Accuracy = 0.6000\n",
      "Epoch 109: Train Loss = 1.4635, Val Loss = 1.2265, Val Accuracy = 0.6000\n",
      "Epoch 110: Train Loss = 1.2912, Val Loss = 1.4744, Val Accuracy = 0.4964\n",
      "Epoch 111: Train Loss = 1.5300, Val Loss = 1.4458, Val Accuracy = 0.5179\n",
      "Epoch 112: Train Loss = 1.7116, Val Loss = 1.4038, Val Accuracy = 0.5786\n",
      "Epoch 113: Train Loss = 1.6096, Val Loss = 1.3752, Val Accuracy = 0.4821\n",
      "Epoch 114: Train Loss = 1.4110, Val Loss = 1.2118, Val Accuracy = 0.5679\n",
      "Epoch 115: Train Loss = 1.2594, Val Loss = 1.3750, Val Accuracy = 0.5571\n",
      "Epoch 116: Train Loss = 1.4006, Val Loss = 1.2377, Val Accuracy = 0.5821\n",
      "Epoch 117: Train Loss = 1.2896, Val Loss = 1.1866, Val Accuracy = 0.4857\n",
      "Epoch 118: Train Loss = 1.2514, Val Loss = 1.1957, Val Accuracy = 0.4964\n",
      "Epoch 119: Train Loss = 1.2763, Val Loss = 1.2256, Val Accuracy = 0.6000\n",
      "Epoch 120: Train Loss = 1.3782, Val Loss = 1.0987, Val Accuracy = 0.6214\n",
      "Epoch 121: Train Loss = 1.1704, Val Loss = 1.2657, Val Accuracy = 0.6143\n",
      "Epoch 122: Train Loss = 1.3797, Val Loss = 1.3560, Val Accuracy = 0.5929\n",
      "Epoch 123: Train Loss = 1.3724, Val Loss = 1.4533, Val Accuracy = 0.5929\n",
      "Epoch 124: Train Loss = 1.4264, Val Loss = 1.3078, Val Accuracy = 0.5929\n",
      "Epoch 125: Train Loss = 1.3229, Val Loss = 1.2166, Val Accuracy = 0.6286\n",
      "Epoch 126: Train Loss = 1.3049, Val Loss = 1.1330, Val Accuracy = 0.6071\n",
      "Epoch 127: Train Loss = 1.1783, Val Loss = 1.1250, Val Accuracy = 0.6071\n",
      "Epoch 128: Train Loss = 1.2352, Val Loss = 1.1905, Val Accuracy = 0.6393\n",
      "Epoch 129: Train Loss = 1.2611, Val Loss = 1.1090, Val Accuracy = 0.6071\n",
      "Epoch 130: Train Loss = 1.1724, Val Loss = 1.1391, Val Accuracy = 0.6071\n",
      "Epoch 131: Train Loss = 1.1948, Val Loss = 1.0988, Val Accuracy = 0.6286\n",
      "Epoch 132: Train Loss = 1.1847, Val Loss = 1.0637, Val Accuracy = 0.6071\n",
      "Epoch 133: Train Loss = 1.1240, Val Loss = 1.0635, Val Accuracy = 0.6286\n",
      "Epoch 134: Train Loss = 1.1379, Val Loss = 1.0905, Val Accuracy = 0.6286\n",
      "Epoch 135: Train Loss = 1.1431, Val Loss = 1.1339, Val Accuracy = 0.6071\n",
      "Epoch 136: Train Loss = 1.1810, Val Loss = 1.0644, Val Accuracy = 0.6107\n",
      "Epoch 137: Train Loss = 1.1535, Val Loss = 1.0481, Val Accuracy = 0.6286\n",
      "Epoch 138: Train Loss = 1.1345, Val Loss = 1.0981, Val Accuracy = 0.6071\n",
      "Epoch 139: Train Loss = 1.1691, Val Loss = 1.0982, Val Accuracy = 0.6393\n",
      "Epoch 140: Train Loss = 1.1701, Val Loss = 1.1147, Val Accuracy = 0.6143\n",
      "Epoch 141: Train Loss = 1.1894, Val Loss = 1.0359, Val Accuracy = 0.6357\n",
      "Epoch 142: Train Loss = 1.1120, Val Loss = 1.0404, Val Accuracy = 0.6357\n",
      "Epoch 143: Train Loss = 1.1166, Val Loss = 1.0941, Val Accuracy = 0.6143\n",
      "Epoch 144: Train Loss = 1.1483, Val Loss = 1.0610, Val Accuracy = 0.6357\n",
      "Epoch 145: Train Loss = 1.1306, Val Loss = 1.0561, Val Accuracy = 0.6143\n",
      "Epoch 146: Train Loss = 1.1250, Val Loss = 1.0259, Val Accuracy = 0.6357\n",
      "Epoch 147: Train Loss = 1.1070, Val Loss = 1.0305, Val Accuracy = 0.6143\n",
      "Epoch 148: Train Loss = 1.0927, Val Loss = 1.0003, Val Accuracy = 0.6143\n",
      "Epoch 149: Train Loss = 1.0728, Val Loss = 1.0014, Val Accuracy = 0.6357\n",
      "Epoch 150: Train Loss = 1.0730, Val Loss = 1.0287, Val Accuracy = 0.6143\n",
      "Epoch 151: Train Loss = 1.0957, Val Loss = 1.0697, Val Accuracy = 0.6357\n",
      "Epoch 152: Train Loss = 1.1471, Val Loss = 1.1408, Val Accuracy = 0.6143\n",
      "Epoch 153: Train Loss = 1.1905, Val Loss = 1.0501, Val Accuracy = 0.6143\n",
      "Epoch 154: Train Loss = 1.1141, Val Loss = 1.0960, Val Accuracy = 0.6179\n",
      "Epoch 155: Train Loss = 1.1931, Val Loss = 1.0697, Val Accuracy = 0.6143\n",
      "Epoch 156: Train Loss = 1.1513, Val Loss = 1.0018, Val Accuracy = 0.6143\n",
      "Epoch 157: Train Loss = 1.0633, Val Loss = 1.0498, Val Accuracy = 0.6357\n",
      "Epoch 158: Train Loss = 1.1227, Val Loss = 1.2076, Val Accuracy = 0.6143\n",
      "Epoch 159: Train Loss = 1.2938, Val Loss = 1.0697, Val Accuracy = 0.6143\n",
      "Epoch 160: Train Loss = 1.1144, Val Loss = 1.1759, Val Accuracy = 0.6179\n",
      "Epoch 161: Train Loss = 1.2885, Val Loss = 1.0365, Val Accuracy = 0.6143\n",
      "Epoch 162: Train Loss = 1.0868, Val Loss = 1.1182, Val Accuracy = 0.6143\n",
      "Epoch 163: Train Loss = 1.2051, Val Loss = 1.1213, Val Accuracy = 0.5107\n",
      "Epoch 164: Train Loss = 1.1905, Val Loss = 1.0615, Val Accuracy = 0.6143\n",
      "Epoch 165: Train Loss = 1.1373, Val Loss = 0.9903, Val Accuracy = 0.6143\n",
      "Epoch 166: Train Loss = 1.0611, Val Loss = 1.0311, Val Accuracy = 0.6357\n",
      "Epoch 167: Train Loss = 1.1243, Val Loss = 1.0841, Val Accuracy = 0.6143\n",
      "Epoch 168: Train Loss = 1.1284, Val Loss = 0.9761, Val Accuracy = 0.6143\n",
      "Epoch 169: Train Loss = 1.0311, Val Loss = 1.0687, Val Accuracy = 0.6357\n",
      "Epoch 170: Train Loss = 1.1331, Val Loss = 1.1703, Val Accuracy = 0.6143\n",
      "Epoch 171: Train Loss = 1.2890, Val Loss = 1.0279, Val Accuracy = 0.6143\n",
      "Epoch 172: Train Loss = 1.0857, Val Loss = 1.4688, Val Accuracy = 0.4929\n",
      "Epoch 173: Train Loss = 1.5964, Val Loss = 1.1637, Val Accuracy = 0.6143\n",
      "Epoch 174: Train Loss = 1.2056, Val Loss = 1.5038, Val Accuracy = 0.6571\n",
      "Epoch 175: Train Loss = 1.5292, Val Loss = 1.2441, Val Accuracy = 0.6143\n",
      "Epoch 176: Train Loss = 1.2490, Val Loss = 1.0498, Val Accuracy = 0.6179\n",
      "Epoch 177: Train Loss = 1.1713, Val Loss = 1.0328, Val Accuracy = 0.6357\n",
      "Epoch 178: Train Loss = 1.1098, Val Loss = 1.2859, Val Accuracy = 0.5857\n",
      "Epoch 179: Train Loss = 1.4023, Val Loss = 1.1203, Val Accuracy = 0.6143\n",
      "Epoch 180: Train Loss = 1.2520, Val Loss = 1.4866, Val Accuracy = 0.5107\n",
      "Epoch 181: Train Loss = 1.5554, Val Loss = 1.0219, Val Accuracy = 0.6143\n",
      "Epoch 182: Train Loss = 1.0898, Val Loss = 1.3595, Val Accuracy = 0.6571\n",
      "Epoch 183: Train Loss = 1.3746, Val Loss = 1.2529, Val Accuracy = 0.6143\n",
      "Epoch 184: Train Loss = 1.2665, Val Loss = 1.0200, Val Accuracy = 0.6179\n",
      "Epoch 185: Train Loss = 1.1545, Val Loss = 1.1086, Val Accuracy = 0.6179\n",
      "Epoch 186: Train Loss = 1.2165, Val Loss = 1.2470, Val Accuracy = 0.6143\n",
      "Epoch 187: Train Loss = 1.3716, Val Loss = 1.2063, Val Accuracy = 0.6143\n",
      "Epoch 188: Train Loss = 1.3313, Val Loss = 1.2198, Val Accuracy = 0.5107\n",
      "Epoch 189: Train Loss = 1.2714, Val Loss = 0.9827, Val Accuracy = 0.6143\n",
      "Epoch 190: Train Loss = 1.0222, Val Loss = 1.1205, Val Accuracy = 0.6143\n",
      "Epoch 191: Train Loss = 1.1472, Val Loss = 0.9689, Val Accuracy = 0.5964\n",
      "Epoch 192: Train Loss = 1.0537, Val Loss = 1.0118, Val Accuracy = 0.6179\n",
      "Epoch 193: Train Loss = 1.1038, Val Loss = 1.1425, Val Accuracy = 0.6143\n",
      "Epoch 194: Train Loss = 1.1887, Val Loss = 0.9407, Val Accuracy = 0.6143\n",
      "Epoch 195: Train Loss = 1.0151, Val Loss = 1.2504, Val Accuracy = 0.4929\n",
      "Epoch 196: Train Loss = 1.3410, Val Loss = 1.0551, Val Accuracy = 0.6143\n",
      "Epoch 197: Train Loss = 1.0792, Val Loss = 1.2418, Val Accuracy = 0.6143\n",
      "Epoch 198: Train Loss = 1.2417, Val Loss = 1.0223, Val Accuracy = 0.6143\n",
      "Epoch 199: Train Loss = 1.0791, Val Loss = 1.2675, Val Accuracy = 0.4929\n",
      "Epoch 200: Train Loss = 1.3674, Val Loss = 1.0820, Val Accuracy = 0.6143\n"
     ]
    }
   ],
   "source": [
    "model = train_gcn(adj, features, labels, idx_train, idx_val, nfeat=features.shape[1], nhid=16, nclass=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b9a63181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Classification Report:\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| Class        |   Precision |   Recall |   F1-Score |   Support |\n",
      "+==============+=============+==========+============+===========+\n",
      "| 0            |        1    |     0.95 |       0.97 |        40 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| 1            |        0.45 |     0.85 |       0.59 |        40 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| 2            |        0.37 |     0.28 |       0.31 |        40 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| 3            |        0.38 |     0.07 |       0.12 |        40 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| 4            |        0.81 |     0.33 |       0.46 |        40 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| 5            |        0.67 |     1    |       0.8  |        40 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| 6            |        0.52 |     0.68 |       0.59 |        40 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| macro avg    |        0.6  |     0.59 |       0.55 |       280 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "| weighted avg |        0.6  |     0.59 |       0.55 |       280 |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "Accuracy: 0.5929\n"
     ]
    }
   ],
   "source": [
    "evaluate_gcn(model, adj, features, labels, idx_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mixmatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
